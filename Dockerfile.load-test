# Load Testing Container for Music Gen AI Staging
# Comprehensive performance and stress testing

FROM python:3.11-slim

LABEL maintainer="Music Gen AI Team"
LABEL description="Load testing container with Locust, Artillery, and custom test suites"

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    wget \
    jq \
    bc \
    netcat-traditional \
    iputils-ping \
    dnsutils \
    && rm -rf /var/lib/apt/lists/*

# Install Node.js for Artillery
RUN curl -fsSL https://deb.nodesource.com/setup_18.x | bash - \
    && apt-get install -y nodejs

# Set working directory
WORKDIR /app

# Copy requirements and install Python dependencies
COPY requirements-test.txt .
RUN pip install --no-cache-dir -r requirements-test.txt

# Install additional load testing tools
RUN pip install \
    locust==2.17.0 \
    requests==2.31.0 \
    websockets==11.0.3 \
    asyncio-mqtt==0.13.0 \
    psutil==5.9.6 \
    matplotlib==3.8.2 \
    seaborn==0.13.0 \
    pandas==2.1.3 \
    numpy==1.24.3

# Install Artillery and other Node.js tools
RUN npm install -g \
    artillery@2.0.0 \
    artillery-plugin-metrics-by-endpoint \
    artillery-plugin-prometheus

# Create directories
RUN mkdir -p /app/tests/load /app/results /app/reports /app/scripts

# Copy test files
COPY tests/load/ /app/tests/load/
COPY scripts/load_testing/ /app/scripts/

# Copy custom load testing scripts
COPY <<EOF /app/scripts/comprehensive_load_test.py
#!/usr/bin/env python3
"""
Comprehensive Load Testing Suite for Music Gen AI
Includes API testing, WebSocket testing, and performance monitoring
"""

import asyncio
import json
import time
import requests
import websockets
import subprocess
import psutil
import pandas as pd
import matplotlib.pyplot as plt
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, asdict
from typing import List, Dict, Any
import statistics

@dataclass
class TestResult:
    test_name: str
    start_time: float
    end_time: float
    duration: float
    status_code: int
    response_time: float
    success: bool
    error_message: str = ""
    response_size: int = 0

class LoadTestSuite:
    def __init__(self, base_url: str, api_key: str = "staging_api_key_change_me"):
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self.results: List[TestResult] = []
        self.session = requests.Session()
        self.session.headers.update({
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json',
            'User-Agent': 'LoadTestSuite/1.0'
        })

    def test_health_endpoint(self) -> TestResult:
        """Test health check endpoint"""
        start_time = time.time()
        try:
            response = self.session.get(f"{self.base_url}/health", timeout=10)
            end_time = time.time()
            
            return TestResult(
                test_name="health_check",
                start_time=start_time,
                end_time=end_time,
                duration=end_time - start_time,
                status_code=response.status_code,
                response_time=response.elapsed.total_seconds(),
                success=response.status_code == 200,
                response_size=len(response.content)
            )
        except Exception as e:
            end_time = time.time()
            return TestResult(
                test_name="health_check",
                start_time=start_time,
                end_time=end_time,
                duration=end_time - start_time,
                status_code=0,
                response_time=0,
                success=False,
                error_message=str(e)
            )

    def test_api_generation(self, prompt: str = "upbeat jazz music") -> TestResult:
        """Test music generation API"""
        start_time = time.time()
        try:
            payload = {
                "prompt": prompt,
                "duration": 10.0,
                "temperature": 0.8,
                "format": "wav"
            }
            
            response = self.session.post(
                f"{self.base_url}/api/v1/generate",
                json=payload,
                timeout=120
            )
            end_time = time.time()
            
            return TestResult(
                test_name="api_generation",
                start_time=start_time,
                end_time=end_time,
                duration=end_time - start_time,
                status_code=response.status_code,
                response_time=response.elapsed.total_seconds(),
                success=response.status_code in [200, 202],
                response_size=len(response.content)
            )
        except Exception as e:
            end_time = time.time()
            return TestResult(
                test_name="api_generation",
                start_time=start_time,
                end_time=end_time,
                duration=end_time - start_time,
                status_code=0,
                response_time=0,
                success=False,
                error_message=str(e)
            )

    async def test_websocket_connection(self) -> TestResult:
        """Test WebSocket connection and streaming"""
        start_time = time.time()
        try:
            ws_url = self.base_url.replace('http', 'ws') + '/ws/generate'
            
            async with websockets.connect(ws_url) as websocket:
                # Send generation request
                request = {
                    "prompt": "electronic dance music",
                    "duration": 5.0,
                    "stream": True
                }
                await websocket.send(json.dumps(request))
                
                # Receive response
                response = await websocket.recv()
                data = json.loads(response)
                
                end_time = time.time()
                
                return TestResult(
                    test_name="websocket_streaming",
                    start_time=start_time,
                    end_time=end_time,
                    duration=end_time - start_time,
                    status_code=200,
                    response_time=end_time - start_time,
                    success=data.get('status') == 'success',
                    response_size=len(response)
                )
        except Exception as e:
            end_time = time.time()
            return TestResult(
                test_name="websocket_streaming",
                start_time=start_time,
                end_time=end_time,
                duration=end_time - start_time,
                status_code=0,
                response_time=0,
                success=False,
                error_message=str(e)
            )

    def run_concurrent_tests(self, num_users: int = 10, duration_seconds: int = 60):
        """Run concurrent load tests"""
        print(f"Starting load test with {num_users} concurrent users for {duration_seconds} seconds")
        
        start_time = time.time()
        end_time = start_time + duration_seconds
        
        def worker():
            worker_results = []
            while time.time() < end_time:
                # Mix of different test types
                test_type = time.time() % 3
                if test_type < 1:
                    result = self.test_health_endpoint()
                elif test_type < 2:
                    result = self.test_api_generation()
                else:
                    result = asyncio.run(self.test_websocket_connection())
                
                worker_results.append(result)
                time.sleep(0.1)  # Small delay between requests
            
            return worker_results

        with ThreadPoolExecutor(max_workers=num_users) as executor:
            futures = [executor.submit(worker) for _ in range(num_users)]
            
            for future in as_completed(futures):
                self.results.extend(future.result())

    def generate_report(self, output_file: str = "/app/results/load_test_report.json"):
        """Generate comprehensive test report"""
        if not self.results:
            return
        
        # Calculate statistics
        successful_tests = [r for r in self.results if r.success]
        failed_tests = [r for r in self.results if not r.success]
        
        response_times = [r.response_time for r in successful_tests]
        
        report = {
            "summary": {
                "total_requests": len(self.results),
                "successful_requests": len(successful_tests),
                "failed_requests": len(failed_tests),
                "success_rate": len(successful_tests) / len(self.results) * 100,
                "test_duration": max([r.end_time for r in self.results]) - min([r.start_time for r in self.results])
            },
            "performance": {
                "avg_response_time": statistics.mean(response_times) if response_times else 0,
                "median_response_time": statistics.median(response_times) if response_times else 0,
                "p95_response_time": self.percentile(response_times, 95) if response_times else 0,
                "p99_response_time": self.percentile(response_times, 99) if response_times else 0,
                "min_response_time": min(response_times) if response_times else 0,
                "max_response_time": max(response_times) if response_times else 0,
            },
            "test_breakdown": {},
            "errors": {}
        }
        
        # Breakdown by test type
        for test_name in set(r.test_name for r in self.results):
            test_results = [r for r in self.results if r.test_name == test_name]
            successful = [r for r in test_results if r.success]
            
            report["test_breakdown"][test_name] = {
                "total": len(test_results),
                "successful": len(successful),
                "failed": len(test_results) - len(successful),
                "success_rate": len(successful) / len(test_results) * 100,
                "avg_response_time": statistics.mean([r.response_time for r in successful]) if successful else 0
            }
        
        # Error analysis
        for result in failed_tests:
            error_key = result.error_message or f"HTTP_{result.status_code}"
            if error_key not in report["errors"]:
                report["errors"][error_key] = 0
            report["errors"][error_key] += 1
        
        # Save report
        with open(output_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"Load test report saved to {output_file}")
        self.print_summary(report)

    def percentile(self, data: List[float], percentile: int) -> float:
        """Calculate percentile"""
        if not data:
            return 0
        sorted_data = sorted(data)
        index = int((percentile / 100.0) * len(sorted_data))
        return sorted_data[min(index, len(sorted_data) - 1)]

    def print_summary(self, report: Dict[str, Any]):
        """Print test summary to console"""
        print("\n" + "="*50)
        print("LOAD TEST SUMMARY")
        print("="*50)
        print(f"Total Requests: {report['summary']['total_requests']}")
        print(f"Successful: {report['summary']['successful_requests']}")
        print(f"Failed: {report['summary']['failed_requests']}")
        print(f"Success Rate: {report['summary']['success_rate']:.2f}%")
        print(f"Test Duration: {report['summary']['test_duration']:.2f}s")
        print()
        print("PERFORMANCE METRICS:")
        print(f"Average Response Time: {report['performance']['avg_response_time']:.3f}s")
        print(f"95th Percentile: {report['performance']['p95_response_time']:.3f}s")
        print(f"99th Percentile: {report['performance']['p99_response_time']:.3f}s")
        print("="*50)

if __name__ == "__main__":
    import sys
    
    base_url = sys.argv[1] if len(sys.argv) > 1 else "http://nginx-staging"
    num_users = int(sys.argv[2]) if len(sys.argv) > 2 else 10
    duration = int(sys.argv[3]) if len(sys.argv) > 3 else 300  # 5 minutes default
    
    suite = LoadTestSuite(base_url)
    suite.run_concurrent_tests(num_users, duration)
    suite.generate_report()
EOF

# Make script executable
RUN chmod +x /app/scripts/comprehensive_load_test.py

# Copy Artillery configuration
COPY <<EOF /app/artillery-config.yml
config:
  target: 'http://nginx-staging'
  phases:
    - duration: 60
      arrivalRate: 1
      name: "Warm up"
    - duration: 300
      arrivalRate: 5
      name: "Ramp up load"
    - duration: 600
      arrivalRate: 10
      name: "Sustained load"
    - duration: 300
      arrivalRate: 20
      name: "Peak load"
    - duration: 120
      arrivalRate: 5
      name: "Cool down"
  processor: "./processors.js"
  plugins:
    metrics-by-endpoint:
      useOnlyRequestNames: true
    prometheus:
      pushgateway:
        url: "http://prometheus-staging:9091"
        jobName: "artillery"

scenarios:
  - name: "Health Check"
    weight: 30
    flow:
      - get:
          url: "/health"
          name: "health_check"

  - name: "API Generation"
    weight: 50
    flow:
      - post:
          url: "/api/v1/generate"
          name: "music_generation"
          headers:
            Authorization: "Bearer staging_api_key_change_me"
          json:
            prompt: "{{ $randomString() }} music"
            duration: 10.0
            temperature: 0.8

  - name: "WebSocket Connection"
    weight: 20
    engine: ws
    flow:
      - connect:
          url: "ws://nginx-staging/ws/generate"
      - send:
          payload: |
            {
              "prompt": "electronic dance music",
              "duration": 5.0,
              "stream": true
            }
      - receive:
          match: "status"
EOF

# Copy monitoring script
COPY <<EOF /app/scripts/monitor_system.py
#!/usr/bin/env python3
"""
System monitoring during load tests
Tracks CPU, memory, network, and application metrics
"""

import time
import json
import psutil
import requests
from datetime import datetime
from typing import Dict, List

class SystemMonitor:
    def __init__(self, prometheus_url: str = "http://prometheus-staging:9090"):
        self.prometheus_url = prometheus_url
        self.metrics: List[Dict] = []

    def collect_system_metrics(self) -> Dict:
        """Collect current system metrics"""
        return {
            "timestamp": datetime.now().isoformat(),
            "cpu_percent": psutil.cpu_percent(interval=1),
            "memory_percent": psutil.virtual_memory().percent,
            "disk_usage": psutil.disk_usage('/').percent,
            "network_io": psutil.net_io_counters()._asdict(),
            "load_average": psutil.getloadavg(),
        }

    def collect_application_metrics(self) -> Dict:
        """Collect application-specific metrics from Prometheus"""
        try:
            # Query Prometheus for application metrics
            queries = {
                "response_time": 'rate(http_request_duration_seconds_sum[1m]) / rate(http_request_duration_seconds_count[1m])',
                "request_rate": 'rate(http_requests_total[1m])',
                "error_rate": 'rate(http_requests_total{status=~"5.."}[1m]) / rate(http_requests_total[1m])',
                "active_connections": 'nginx_active_connections',
                "generation_queue_size": 'celery_queue_length{queue="generation"}',
            }
            
            metrics = {}
            for name, query in queries.items():
                try:
                    response = requests.get(
                        f"{self.prometheus_url}/api/v1/query",
                        params={"query": query},
                        timeout=5
                    )
                    if response.status_code == 200:
                        data = response.json()
                        if data.get("data", {}).get("result"):
                            metrics[name] = float(data["data"]["result"][0]["value"][1])
                        else:
                            metrics[name] = 0
                    else:
                        metrics[name] = 0
                except:
                    metrics[name] = 0
            
            return metrics
        except:
            return {}

    def monitor_duration(self, duration_seconds: int = 3600):
        """Monitor system for specified duration"""
        print(f"Starting system monitoring for {duration_seconds} seconds")
        
        start_time = time.time()
        end_time = start_time + duration_seconds
        
        while time.time() < end_time:
            system_metrics = self.collect_system_metrics()
            app_metrics = self.collect_application_metrics()
            
            combined_metrics = {**system_metrics, **app_metrics}
            self.metrics.append(combined_metrics)
            
            print(f"CPU: {system_metrics['cpu_percent']:.1f}% | "
                  f"Memory: {system_metrics['memory_percent']:.1f}% | "
                  f"Requests/s: {app_metrics.get('request_rate', 0):.1f}")
            
            time.sleep(10)  # Collect metrics every 10 seconds

    def save_metrics(self, filename: str = "/app/results/system_metrics.json"):
        """Save collected metrics to file"""
        with open(filename, 'w') as f:
            json.dump(self.metrics, f, indent=2)
        print(f"System metrics saved to {filename}")

if __name__ == "__main__":
    import sys
    duration = int(sys.argv[1]) if len(sys.argv) > 1 else 3600
    
    monitor = SystemMonitor()
    monitor.monitor_duration(duration)
    monitor.save_metrics()
EOF

RUN chmod +x /app/scripts/monitor_system.py

# Default command
CMD ["python", "/app/scripts/comprehensive_load_test.py"]