# MusicGen AI - System Demonstration & Usage Guide

## üéØ System Overview

The MusicGen AI system is a comprehensive, production-ready text-to-music generation platform with multi-instrument capabilities. Here's how to use it and what it can do.

## üöÄ Getting Started

### 1. Installation

```bash
# Clone the repository
git clone https://github.com/Bright-L01/music-gen-ai.git
cd music-gen-ai

# Create conda environment
conda env create -f environment.yml
conda activate music-gen-ai

# Install in development mode
pip install -e ".[dev]"

# Install pre-commit hooks
pre-commit install
```

### 2. Quick Start

```bash
# Start the API server with web UI
music-gen-api --host 0.0.0.0 --port 8000

# Access the web UI
open http://localhost:8000

# Access the multi-track studio
open http://localhost:8000/studio
```

## üéµ Usage Examples

### Basic Text-to-Music Generation

```python
from music_gen.models.musicgen import create_musicgen_model

# Create model
model = create_musicgen_model("base")

# Generate music from text
audio = model.generate_audio(
    texts=["Upbeat jazz piano with smooth saxophone and walking bass"],
    duration=30.0,
    temperature=1.0
)

# Save the audio
save_audio_file(audio, "jazz_output.wav")
```

### Multi-Instrument Generation

```python
from music_gen.models.multi_instrument import MultiTrackGenerator, TrackGenerationConfig

# Initialize generator
generator = MultiTrackGenerator(model, config)

# Configure tracks
tracks = [
    TrackGenerationConfig(
        instrument="piano",
        volume=0.8,
        pan=0.0,
        reverb=0.3
    ),
    TrackGenerationConfig(
        instrument="bass",
        volume=0.6,
        pan=-0.3,
        start_time=2.0  # Bass enters at 2 seconds
    ),
    TrackGenerationConfig(
        instrument="drums",
        volume=0.7,
        pan=0.0,
        start_time=4.0  # Drums enter at 4 seconds
    ),
    TrackGenerationConfig(
        instrument="saxophone",
        volume=0.7,
        pan=0.3,
        reverb=0.4
    )
]

# Generate multi-track music
result = generator.generate(
    prompt="Smooth jazz quartet playing in a late night club",
    track_configs=tracks,
    duration=60.0,
    temperature=0.9,
    use_beam_search=True
)

# Access individual tracks
piano_audio = result.audio_tracks["piano"]
bass_audio = result.audio_tracks["bass"]

# Get the professionally mixed output
mixed_audio = result.mixed_audio
```

### Professional Audio Mixing

```python
from music_gen.audio.mixing import MixingEngine, MixingConfig, TrackConfig
from music_gen.audio.mixing.effects import Reverb, Compressor, EQ

# Create mixing engine
config = MixingConfig(sample_rate=44100, master_limiter=True)
mixer = MixingEngine(config)

# Configure track settings
track_configs = {
    "piano": TrackConfig(
        name="piano",
        volume=0.8,
        pan=0.0,
        reverb_send=0.3,
        eq_mid_gain=2.0,  # Boost midrange
        compressor_threshold=-15.0,
        compressor_ratio=3.0
    ),
    "bass": TrackConfig(
        name="bass",
        volume=0.6,
        pan=-0.5,
        eq_low_gain=3.0,  # Boost low frequencies
        compressor_threshold=-10.0,
        compressor_ratio=4.0
    ),
    "drums": TrackConfig(
        name="drums",
        volume=0.7,
        pan=0.0,
        compressor_threshold=-12.0,
        compressor_ratio=6.0,
        gate_threshold=-30.0  # Noise gate
    )
}

# Mix the tracks
mixed = mixer.mix(audio_tracks, track_configs, duration=60.0)

# Get metering information
metering = mixer.get_metering(mixed)
print(f"RMS: {metering['rms_db']:.1f} dB")
print(f"Peak: {metering['peak_db']:.1f} dB")
print(f"LUFS: {metering['lufs']:.1f}")
```

### Track Separation

```python
from music_gen.audio.separation import HybridSeparator

# Initialize separator
separator = HybridSeparator(
    primary_method="demucs",
    secondary_method="spleeter",
    blend_mode="weighted"
)
separator.load_model()

# Separate mixed audio into tracks
result = separator.separate(
    mixed_audio,
    sample_rate=44100,
    targets=["vocals", "drums", "bass", "other"]
)

# Access separated stems
vocals = result.stems["vocals"]
drums = result.stems["drums"]
bass = result.stems["bass"]
other = result.stems["other"]

# Check separation quality
print(f"Separation confidence: {result.confidence_scores}")
```

### MIDI Export

```python
from music_gen.export.midi import MIDIConverter, MIDIExportConfig

# Configure MIDI export
config = MIDIExportConfig(
    tempo=120,
    quantize=True,
    quantize_strength=0.8
)
converter = MIDIConverter(config)

# Convert audio tracks to MIDI
midi = converter.convert(
    audio_tracks={"piano": piano_audio, "bass": bass_audio},
    instrument_configs={
        "piano": {"midi_program": 0},
        "bass": {"midi_program": 33}
    }
)

# Save MIDI file
midi.write("output.mid")
```

## üåê API Usage

### REST API Endpoints

```bash
# List available instruments
curl http://localhost:8000/instruments

# Get instrument details
curl http://localhost:8000/instruments/piano

# Generate multi-instrument music
curl -X POST http://localhost:8000/generate/multi-instrument \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Epic orchestral piece with strings and brass",
    "tracks": [
      {"instrument": "violin", "volume": 0.8},
      {"instrument": "cello", "volume": 0.7},
      {"instrument": "trumpet", "volume": 0.6},
      {"instrument": "french_horn", "volume": 0.5}
    ],
    "duration": 30.0
  }'

# Separate audio tracks
curl -X POST http://localhost:8000/separate-tracks \
  -F "audio_file=@mixed.wav" \
  -F "method=hybrid"

# Export to MIDI
curl -X POST http://localhost:8000/export-midi \
  -F "audio_file=@piano.wav" \
  -F "instrument=piano"
```

### WebSocket Streaming

```javascript
// Connect to streaming endpoint
const ws = new WebSocket('ws://localhost:8000/stream');

// Start streaming generation
ws.send(JSON.stringify({
  type: 'generate',
  prompt: 'Ambient electronic music',
  quality: 'balanced'
}));

// Receive audio chunks
ws.onmessage = (event) => {
  const data = JSON.parse(event.data);
  if (data.type === 'audio_chunk') {
    // Play audio chunk
    playAudioChunk(data.audio);
  }
};
```

## üéõÔ∏è Web UI Features

### Main Generation Interface
- Text prompt input with suggestions
- Musical attribute controls (genre, mood, tempo)
- Generation parameters (temperature, sampling)
- Real-time audio playback
- Download options

### Multi-Track Studio
- Professional DAW-style interface
- Up to 8 simultaneous tracks
- Per-track controls:
  - Volume and pan
  - Effects sends (reverb, delay)
  - Solo/mute
  - Timing controls
- Visual waveform display
- Transport controls
- Export options (mixed, stems, MIDI)

## üìä System Capabilities

### Supported Instruments (30+)
- **Keyboards**: Piano, Electric Piano, Harpsichord, Organ, Synthesizer
- **Strings**: Violin, Viola, Cello, Double Bass, Harp
- **Guitars**: Acoustic, Electric, Bass
- **Brass**: Trumpet, Trombone, French Horn, Tuba
- **Woodwinds**: Flute, Clarinet, Saxophone, Oboe
- **Percussion**: Drums, Timpani, Xylophone, Vibraphone
- **Voice**: Soprano, Alto, Tenor, Bass, Choir
- **Synths**: Pad, Lead

### Audio Effects
- **EQ**: Parametric with low/mid/high bands
- **Compressor**: With attack, release, ratio controls
- **Reverb**: Room size, damping, mix controls
- **Delay**: Time, feedback, modulation
- **Chorus**: Voices, depth, rate
- **Limiter**: Threshold, release
- **Gate**: Threshold, attack, release
- **Distortion**: Drive, tone, output gain

### Performance Metrics
- **Generation Speed**: ~5x real-time on GPU
- **Streaming Latency**: <500ms
- **Audio Quality**: 44.1kHz, 24-bit
- **Max Duration**: 120 seconds per generation
- **Concurrent Sessions**: 5+ supported

## üîß Configuration Options

### Model Selection
```python
# Available models
models = ["musicgen-small", "musicgen-base", "musicgen-large"]

# Multi-instrument configuration
config = MultiInstrumentConfig(
    num_instruments=32,
    instrument_embedding_dim=256,
    max_tracks=8,
    use_instrument_attention=True,
    parallel_generation=True
)
```

### Training Configuration
```yaml
# configs/training/base.yaml
training:
  batch_size: 16
  learning_rate: 1e-4
  num_epochs: 100
  gradient_accumulation_steps: 4
  
model:
  hidden_size: 1024
  num_layers: 24
  num_attention_heads: 16
```

## üê≥ Docker Deployment

```bash
# Build production image
docker build -t music-gen-ai:prod -f Dockerfile.prod .

# Run with GPU support
docker run --gpus all -p 8000:8000 music-gen-ai:prod

# Using Docker Compose
docker-compose up -d
```

## üìà Monitoring & Logging

The system includes comprehensive logging and monitoring:

```python
# Access generation metrics
from music_gen.evaluation.metrics import AudioQualityMetrics

metrics = AudioQualityMetrics()
quality = metrics.evaluate_audio_quality([generated_audio])

print(f"SNR: {quality['snr_mean']:.2f} dB")
print(f"Harmonic/Percussive Ratio: {quality['harmonic_percussive_ratio']:.2f}")
print(f"Tempo Stability: {quality['tempo_stability']:.2f}")
```

## üß™ Testing

```bash
# Run all tests
pytest

# Run specific test categories
pytest -m unit          # Unit tests
pytest -m integration   # Integration tests
pytest -m e2e          # End-to-end tests

# Run with coverage
pytest --cov=music_gen --cov-report=html

# Test multi-instrument features
pytest tests/test_multi_instrument.py -v
```

## üéØ Example Outputs

### Simple Generation
**Prompt**: "Peaceful piano melody with gentle strings"
- **Output**: 30-second instrumental piece
- **Format**: WAV, 44.1kHz, stereo
- **Size**: ~5.2 MB

### Multi-Track Generation
**Prompt**: "Jazz quartet in a smoky club"
- **Tracks Generated**: Piano, Bass, Drums, Saxophone
- **Individual Stems**: 4 separate WAV files
- **Mixed Output**: Professionally balanced mix
- **MIDI Export**: 4-track MIDI file

### Real-time Streaming
**Prompt**: "Electronic dance music with heavy bass"
- **Latency**: 450ms to first audio
- **Chunk Size**: 0.5 seconds
- **Quality**: Balanced mode (good quality/speed trade-off)

## üöÄ Future Capabilities

The system is designed for extensibility:
- VST plugin integration
- Cloud-based rendering
- Collaborative features
- Mobile app support
- Advanced AI features (style transfer, humanization)

---

This comprehensive system provides professional-grade music generation with extensive customization options, making it suitable for musicians, producers, content creators, and developers building music applications.