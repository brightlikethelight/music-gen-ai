# Logstash configuration for Music Gen AI structured logs
# Processes JSON logs and forwards to Elasticsearch with proper indexing

input {
  # File input for application logs
  file {
    path => "/var/log/musicgen/app.log"
    start_position => "beginning"
    sincedb_path => "/var/lib/logstash/sincedb_app"
    codec => "json"
    tags => ["application", "musicgen"]
    type => "application"
  }

  # File input for audit logs
  file {
    path => "/var/log/musicgen/audit.log"
    start_position => "beginning"
    sincedb_path => "/var/lib/logstash/sincedb_audit"
    codec => "json"
    tags => ["audit", "security", "musicgen"]
    type => "audit"
  }

  # File input for performance logs
  file {
    path => "/var/log/musicgen/performance.log"
    start_position => "beginning"
    sincedb_path => "/var/lib/logstash/sincedb_performance"
    codec => "json"
    tags => ["performance", "metrics", "musicgen"]
    type => "performance"
  }

  # File input for error logs
  file {
    path => "/var/log/musicgen/error.log"
    start_position => "beginning"
    sincedb_path => "/var/lib/logstash/sincedb_error"
    codec => "json"
    tags => ["error", "critical", "musicgen"]
    type => "error"
  }

  # Beats input for Filebeat integration
  beats {
    port => 5044
    codec => "json"
  }

  # HTTP input for direct log shipping
  http {
    port => 8080
    codec => "json"
    tags => ["http", "direct"]
  }

  # Syslog input for system logs
  syslog {
    port => 514
    tags => ["syslog", "system"]
  }
}

filter {
  # Parse timestamp if it's a string
  if [timestamp] {
    date {
      match => [ "timestamp", "ISO8601" ]
      target => "@timestamp"
    }
  }

  # Add service metadata
  mutate {
    add_field => {
      "service.name" => "musicgen-api"
      "service.version" => "${SERVICE_VERSION:1.0.0}"
      "service.environment" => "${ENVIRONMENT:production}"
    }
  }

  # Process application logs
  if [type] == "application" {
    # Extract log level
    if [levelname] {
      mutate {
        rename => { "levelname" => "log.level" }
      }
    }

    # Extract logger name
    if [name] {
      mutate {
        rename => { "name" => "log.logger" }
      }
    }

    # Extract correlation ID for distributed tracing
    if [correlation_id] {
      mutate {
        add_field => { "trace.correlation_id" => "%{correlation_id}" }
      }
    }

    # Extract OpenTelemetry trace fields
    if [trace_id] {
      mutate {
        add_field => { "trace.id" => "%{trace_id}" }
      }
    }

    if [span_id] {
      mutate {
        add_field => { "span.id" => "%{span_id}" }
      }
    }

    # Parse stack traces for errors
    if [log.level] == "ERROR" and [message] =~ /Traceback/ {
      mutate {
        add_tag => ["exception", "stacktrace"]
      }
    }
  }

  # Process audit logs
  if [type] == "audit" {
    # Extract user information
    if [user_id] {
      mutate {
        add_field => { "user.id" => "%{user_id}" }
      }
    }

    # Extract IP address for security analysis
    if [ip_address] {
      mutate {
        add_field => { "source.ip" => "%{ip_address}" }
      }
    }

    # Extract authentication events
    if [event] {
      mutate {
        add_field => { "event.action" => "%{event}" }
      }
    }

    # Mark failed authentication attempts
    if [success] == false and [event] =~ /login|authentication/ {
      mutate {
        add_tag => ["security_alert", "failed_auth"]
        add_field => { "event.outcome" => "failure" }
      }
    }

    # GeoIP lookup for IP addresses
    if [source.ip] and [source.ip] !~ /^(10\.|172\.|192\.168\.|127\.)/ {
      geoip {
        source => "source.ip"
        target => "source.geo"
      }
    }
  }

  # Process performance logs
  if [type] == "performance" {
    # Extract HTTP method and path
    if [method] {
      mutate {
        add_field => { "http.request.method" => "%{method}" }
      }
    }

    if [path] {
      mutate {
        add_field => { "url.path" => "%{path}" }
      }
    }

    # Extract response metrics
    if [status_code] {
      mutate {
        add_field => { "http.response.status_code" => "%{status_code}" }
      }
    }

    if [duration_ms] {
      mutate {
        convert => { "duration_ms" => "float" }
        add_field => { "http.response.time" => "%{duration_ms}" }
      }
    }

    # Classify response times
    if [duration_ms] {
      if [duration_ms] < 100 {
        mutate { add_tag => ["fast_response"] }
      } else if [duration_ms] < 1000 {
        mutate { add_tag => ["normal_response"] }
      } else if [duration_ms] < 5000 {
        mutate { add_tag => ["slow_response"] }
      } else {
        mutate { add_tag => ["very_slow_response", "performance_issue"] }
      }
    }

    # Extract memory usage
    if [memory_usage_mb] {
      mutate {
        convert => { "memory_usage_mb" => "float" }
        add_field => { "system.memory.usage_mb" => "%{memory_usage_mb}" }
      }
    }

    # Extract database metrics
    if [db_query_count] {
      mutate {
        convert => { "db_query_count" => "integer" }
        add_field => { "database.query.count" => "%{db_query_count}" }
      }
    }

    if [db_duration_ms] {
      mutate {
        convert => { "db_duration_ms" => "float" }
        add_field => { "database.query.time" => "%{db_duration_ms}" }
      }
    }
  }

  # Process error logs
  if [type] == "error" {
    mutate {
      add_tag => ["error", "alert"]
      add_field => { "event.severity" => "error" }
    }

    # Extract error codes if present
    if [error_code] {
      mutate {
        add_field => { "error.code" => "%{error_code}" }
      }
    }

    # Extract exception type
    if [exc_info] and [exc_info] != "null" {
      mutate {
        add_tag => ["exception"]
      }
    }
  }

  # Common processing for all log types
  
  # Add environment labels
  mutate {
    add_field => {
      "labels.environment" => "${ENVIRONMENT:production}"
      "labels.datacenter" => "${DATACENTER:unknown}"
      "labels.version" => "${SERVICE_VERSION:1.0.0}"
    }
  }

  # Remove unnecessary fields
  mutate {
    remove_field => ["host", "path", "beat", "input", "prospector", "source"]
  }

  # Add index routing based on log type and date
  mutate {
    add_field => { "[@metadata][index]" => "musicgen-%{type}-%{+YYYY.MM.dd}" }
  }
}

output {
  # Output to Elasticsearch with proper indexing
  elasticsearch {
    hosts => ["${ELASTICSEARCH_HOST:localhost:9200}"]
    index => "%{[@metadata][index]}"
    template_name => "musicgen"
    template_pattern => "musicgen-*"
    template_overwrite => true
    template => "/etc/logstash/templates/musicgen-template.json"
    
    # Enable ILM for automatic index lifecycle management
    ilm_enabled => true
    ilm_rollover_alias => "musicgen"
    ilm_pattern => "{now/d}-000001"
    ilm_policy => "musicgen-policy"
  }

  # Output to file for debugging (remove in production)
  if "${LOGSTASH_DEBUG:false}" == "true" {
    file {
      path => "/var/log/logstash/debug-output-%{type}.log"
      codec => json_lines
    }
  }

  # Output critical errors to separate index
  if "error" in [tags] or [log.level] == "ERROR" {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOST:localhost:9200}"]
      index => "musicgen-errors-%{+YYYY.MM.dd}"
    }
  }

  # Output security events to separate index
  if "security_alert" in [tags] {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOST:localhost:9200}"]
      index => "musicgen-security-%{+YYYY.MM.dd}"
    }
  }

  # Send performance issues to monitoring system
  if "performance_issue" in [tags] {
    # Add webhook or monitoring system output here
    stdout {
      codec => json_lines
    }
  }

  # Debug output to stdout (remove in production)
  if "${LOGSTASH_DEBUG:false}" == "true" {
    stdout {
      codec => rubydebug
    }
  }
}