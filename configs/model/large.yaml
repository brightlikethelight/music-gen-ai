# Large model configuration for maximum quality

# @package _global_
defaults:
  - base

size: large
hidden_size: 1024
num_layers: 24
num_heads: 16
intermediate_size: 4096
max_sequence_length: 8192
max_generation_length: 4096

# Enable memory optimizations for larger model
gradient_checkpointing: true
use_scaled_dot_product_attention: true

# Larger conditioning
conditioning_dim: 768

# May need lower dropout for larger models
attention_dropout: 0.05
hidden_dropout: 0.05